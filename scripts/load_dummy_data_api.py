#!/usr/bin/env python3
"""
Load generated dummy businesses CSV into the running app via HTTP API.

Example:
  python3 scripts/load_dummy_data_api.py \
    --csv data/dummy/businesses.csv \
    --base-url http://localhost:8080 \
    --concurrency 8
"""

from __future__ import annotations

import argparse
import csv
import json
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Optional
from urllib import error, request


@dataclass(frozen=True)
class ImportRow:
    source_id: int
    owner_id: int
    name: str
    category: str
    phone: Optional[str]
    address: str
    latitude: float
    longitude: float


@dataclass
class ImportResult:
    ok: bool
    source_id: int
    created_id: Optional[int] = None
    status: Optional[int] = None
    message: Optional[str] = None


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Load businesses.csv into the app via POST /v1/business"
    )
    parser.add_argument(
        "--csv",
        required=True,
        help="Path to businesses.csv generated by scripts/gen_dummy_data.py",
    )
    parser.add_argument(
        "--base-url",
        default="http://localhost:8080",
        help="App base URL (default: http://localhost:8080)",
    )
    parser.add_argument(
        "--path",
        default="/v1/business",
        help="Create API path (default: /v1/business)",
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=8,
        help="Number of concurrent requests (default: 8)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=0,
        help="Max rows to import, 0 means all (default: 0)",
    )
    parser.add_argument(
        "--timeout-seconds",
        type=float,
        default=5.0,
        help="HTTP request timeout in seconds (default: 5.0)",
    )
    parser.add_argument(
        "--retries",
        type=int,
        default=2,
        help="Retries per failed request (default: 2)",
    )
    parser.add_argument(
        "--progress-every",
        type=int,
        default=1000,
        help="Progress log interval (default: 1000)",
    )
    parser.add_argument(
        "--wait-for-index-sync",
        action="store_true",
        help="Wait until actuator metric proximity.indexsync.backlog becomes 0",
    )
    parser.add_argument(
        "--wait-timeout-seconds",
        type=int,
        default=1800,
        help="Timeout for --wait-for-index-sync (default: 1800)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Validate and count rows without sending HTTP requests",
    )
    return parser.parse_args()


def parse_csv(path: Path, limit: int) -> list[ImportRow]:
    rows: list[ImportRow] = []
    with path.open("r", encoding="utf-8", newline="") as fp:
        reader = csv.DictReader(fp)
        required = {
            "id",
            "owner_id",
            "name",
            "category",
            "phone",
            "address",
            "latitude",
            "longitude",
        }
        if reader.fieldnames is None:
            raise ValueError("CSV header is missing")
        missing = required.difference(set(reader.fieldnames))
        if missing:
            raise ValueError(f"Missing CSV columns: {', '.join(sorted(missing))}")

        for line_idx, row in enumerate(reader, start=2):
            try:
                rows.append(
                    ImportRow(
                        source_id=int(row["id"]),
                        owner_id=int(row["owner_id"]),
                        name=(row["name"] or "").strip(),
                        category=(row["category"] or "").strip(),
                        phone=(row["phone"] or "").strip() or None,
                        address=(row["address"] or "").strip(),
                        latitude=float(row["latitude"]),
                        longitude=float(row["longitude"]),
                    )
                )
            except Exception as exc:  # noqa: BLE001
                raise ValueError(f"Invalid CSV at line {line_idx}: {exc}") from exc

            if limit > 0 and len(rows) >= limit:
                break
    return rows


def post_business(api_url: str, timeout_seconds: float, retries: int, row: ImportRow) -> ImportResult:
    payload = {
        "ownerId": row.owner_id,
        "name": row.name,
        "category": row.category,
        "phone": row.phone,
        "address": row.address,
        "latitude": row.latitude,
        "longitude": row.longitude,
    }
    body = json.dumps(payload).encode("utf-8")

    for attempt in range(retries + 1):
        try:
            req = request.Request(
                api_url,
                data=body,
                method="POST",
                headers={"Content-Type": "application/json"},
            )
            with request.urlopen(req, timeout=timeout_seconds) as resp:
                status = resp.getcode()
                resp_body = resp.read().decode("utf-8", errors="replace")
                if status != 201:
                    return ImportResult(
                        ok=False,
                        source_id=row.source_id,
                        status=status,
                        message=f"unexpected status {status}, body={resp_body[:300]}",
                    )
                created_id = None
                try:
                    parsed = json.loads(resp_body) if resp_body else {}
                    value = parsed.get("id")
                    if value is not None:
                        created_id = int(value)
                except Exception:  # noqa: BLE001
                    created_id = None
                return ImportResult(
                    ok=True,
                    source_id=row.source_id,
                    created_id=created_id,
                    status=status,
                )
        except error.HTTPError as exc:
            err_body = exc.read().decode("utf-8", errors="replace")
            if exc.code >= 500 and attempt < retries:
                time.sleep(min(0.1 * (2 ** attempt), 1.0))
                continue
            return ImportResult(
                ok=False,
                source_id=row.source_id,
                status=exc.code,
                message=err_body[:300] or str(exc),
            )
        except Exception as exc:  # noqa: BLE001
            if attempt < retries:
                time.sleep(min(0.1 * (2 ** attempt), 1.0))
                continue
            return ImportResult(
                ok=False,
                source_id=row.source_id,
                message=str(exc),
            )

    return ImportResult(ok=False, source_id=row.source_id, message="retry exhausted")


def read_backlog_metric(base_url: str, timeout_seconds: float) -> Optional[float]:
    metric_url = f"{base_url.rstrip('/')}/actuator/metrics/proximity.indexsync.backlog"
    req = request.Request(metric_url, method="GET")
    with request.urlopen(req, timeout=timeout_seconds) as resp:
        data = json.loads(resp.read().decode("utf-8", errors="replace"))
        measurements = data.get("measurements") or []
        if not measurements:
            return None
        value = measurements[0].get("value")
        return float(value) if value is not None else None


def wait_for_index_sync(base_url: str, timeout_seconds: int, request_timeout_seconds: float) -> bool:
    deadline = time.time() + timeout_seconds
    while time.time() < deadline:
        try:
            backlog = read_backlog_metric(base_url, request_timeout_seconds)
            if backlog is not None and backlog <= 0.0:
                print("[info] index sync backlog reached 0")
                return True
            if backlog is None:
                print("[warn] backlog metric unavailable yet")
            else:
                print(f"[info] current backlog={backlog:.0f}")
        except Exception as exc:  # noqa: BLE001
            print(f"[warn] failed reading backlog metric: {exc}")
        time.sleep(2.0)
    return False


def main() -> int:
    args = parse_args()
    csv_path = Path(args.csv).expanduser()

    if not csv_path.is_file():
        print(f"[error] CSV not found: {csv_path}", file=sys.stderr)
        return 1

    if args.concurrency <= 0:
        print("[error] --concurrency must be > 0", file=sys.stderr)
        return 1
    if args.retries < 0:
        print("[error] --retries must be >= 0", file=sys.stderr)
        return 1
    if args.limit < 0:
        print("[error] --limit must be >= 0", file=sys.stderr)
        return 1

    try:
        rows = parse_csv(csv_path, args.limit)
    except Exception as exc:  # noqa: BLE001
        print(f"[error] {exc}", file=sys.stderr)
        return 1

    total = len(rows)
    api_url = f"{args.base_url.rstrip('/')}/{args.path.lstrip('/')}"

    print("[info] load started")
    print(f"[info] csv={csv_path}")
    print(f"[info] total_rows={total}")
    print(f"[info] api_url={api_url}")
    print(f"[info] concurrency={args.concurrency}")
    print(f"[info] retries={args.retries}")

    if total == 0:
        print("[done] no rows to import")
        return 0

    if args.dry_run:
        print("[done] dry-run complete (no HTTP requests sent)")
        return 0

    success = 0
    failed = 0
    failures: list[ImportResult] = []
    started = time.time()

    with ThreadPoolExecutor(max_workers=args.concurrency) as executor:
        futures = [
            executor.submit(
                post_business,
                api_url,
                args.timeout_seconds,
                args.retries,
                row,
            )
            for row in rows
        ]

        for idx, future in enumerate(as_completed(futures), start=1):
            result = future.result()
            if result.ok:
                success += 1
            else:
                failed += 1
                if len(failures) < 20:
                    failures.append(result)

            if args.progress_every > 0 and (idx % args.progress_every == 0 or idx == total):
                elapsed = max(time.time() - started, 0.001)
                rps = idx / elapsed
                print(
                    f"[progress] processed={idx}/{total} success={success} failed={failed} "
                    f"elapsed={elapsed:.1f}s rps={rps:.1f}"
                )

    elapsed_total = max(time.time() - started, 0.001)
    print(
        f"[done] import finished processed={total} success={success} failed={failed} "
        f"elapsed={elapsed_total:.1f}s avg_rps={total / elapsed_total:.1f}"
    )

    if failures:
        print("[warn] showing up to first 20 failures:")
        for item in failures:
            print(
                f"  - source_id={item.source_id} status={item.status} message={item.message}"
            )

    if failed > 0:
        return 2

    if args.wait_for_index_sync:
        print("[info] waiting for index sync backlog to reach 0...")
        done = wait_for_index_sync(
            base_url=args.base_url,
            timeout_seconds=args.wait_timeout_seconds,
            request_timeout_seconds=args.timeout_seconds,
        )
        if not done:
            print("[warn] timeout while waiting for index sync backlog")
            return 3

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
